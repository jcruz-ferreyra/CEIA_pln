{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1bTVkfJwhuRz",
    "outputId": "fafe90ca-0b3d-4e4b-b9c3-233d72763362"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: nvidia-smi: command not found\n"
     ]
    }
   ],
   "source": [
    "# Check nvidia is working\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3toDIIC-lFn_"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import io\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4ztzNNrmlMMv"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "from IPython import display\n",
    "\n",
    "# Define home path.\n",
    "HOME = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DV3_1IYIldkn",
    "outputId": "85518c50-2b0c-4991-e897-9a54179bf69d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive account\n",
    "%cd {HOME}\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPE53l08o19L"
   },
   "source": [
    "**Load data as dataframe**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a trabajar con un dataset de alrededor de 2000 canciones de musica folclorica argentina para entrenar un modelo de prediccion del siguiente termino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "95LDbWPWlumw"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/content/drive/MyDrive/CEIA/nlp/data/songs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0VMMY1Jfs-xE"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9BDpRqxeo9fW"
   },
   "source": [
    "**Preprocess text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "6iV7mWlNyCUQ"
   },
   "outputs": [],
   "source": [
    "def process_text(input_string):\n",
    "    # Convert the string to lowercase\n",
    "    processed = input_string.lower()\n",
    "    # Remove text between parentheses (including the parentheses)\n",
    "    processed = re.sub(r'\\(.*?\\)', '', processed)\n",
    "    # Remove every non-alphanumeric and ' character\n",
    "    processed = re.sub(r\"[^a-zA-Z0-9\\s']\", '', processed)\n",
    "    # Replace multiple spaces with a single space\n",
    "    processed = re.sub(r'\\s+', ' ', processed)\n",
    "    # Remove leading and trailing spaces\n",
    "    processed = processed.strip()\n",
    "\n",
    "    return processed\n",
    "\n",
    "df[\"lyrics\"] = df[\"lyrics\"].apply(process_text)\n",
    "\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gwj-DB1_tR07"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2WYu4x9pRSc"
   },
   "source": [
    "**Preprocess sequences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "12ZchR5Cl4Ou"
   },
   "outputs": [],
   "source": [
    "# Preprocess text with keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer # similar to nltk.ltokenizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence # similar to nltk.word_tokenize\n",
    "from tensorflow.keras.utils import pad_sequences # for pad sentences shorter than max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "KQcISixBmEC3",
    "outputId": "4a210d4b-18b8-4760-893f-0571c2c565d2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'estaba el cerro tranquilo cada cual en su trabajo cuando llegaron de abajo tres democratas eternos emisarios del gobierno la lengua como badajo'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert pd.Series to list\n",
    "text = list(df.loc[:, \"lyrics\"])\n",
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jIGlg3kVnswv",
    "outputId": "94f365b8-852a-4973-f935-f9fa0bbf1da0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['estaba',\n",
       " 'el',\n",
       " 'cerro',\n",
       " 'tranquilo',\n",
       " 'cada',\n",
       " 'cual',\n",
       " 'en',\n",
       " 'su',\n",
       " 'trabajo',\n",
       " 'cuando',\n",
       " 'llegaron',\n",
       " 'de',\n",
       " 'abajo',\n",
       " 'tres',\n",
       " 'democratas',\n",
       " 'eternos',\n",
       " 'emisarios',\n",
       " 'del',\n",
       " 'gobierno',\n",
       " 'la',\n",
       " 'lengua',\n",
       " 'como',\n",
       " 'badajo']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Segment text into list of words\n",
    "segmented_sentences = [text_to_word_sequence(sentence) for sentence in text]\n",
    "\n",
    "# Filter sentences longer than 60 words (to reduce training time for the excercise)\n",
    "segmented_sentences = [sentence for sentence in segmented_sentences if len(sentence) < 60]\n",
    "\n",
    "segmented_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "uLKew5QOnzwf",
    "outputId": "fa3ca78c-25be-44bc-97ea-61ff54a1dd52"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get sequences length\n",
    "length_sentences = [len(sentence) for sentence in segmented_sentences]\n",
    "\n",
    "# Plot sequences length\n",
    "plt.hist(length_sentences,bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p0Tr5xZ4pMlP",
    "outputId": "3ae89307-4082-4eea-d68c-c25b27926d85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_context_size: 35\n"
     ]
    }
   ],
   "source": [
    "# We use the value corresponding to the 90 percentile as max context size.\n",
    "max_context_size = int(np.percentile(length_sentences, 90) -1) # -1 because the last is the target value\n",
    "\n",
    "# # Other valid approaches are using mean or median (or any other criterion)\n",
    "# max_context_size = int(np.ceil(np.mean(length_sentences)))\n",
    "# max_context_size = int(np.ceil(np.median(length_sentences)))\n",
    "\n",
    "print(f'max_context_size: {max_context_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8EcVpwkpOkY"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6z-LYbUsmIs"
   },
   "source": [
    "**Split sequences for train and validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "dalSvzLDpXUX"
   },
   "outputs": [],
   "source": [
    "segmented_sentences_train, segmented_sentences_val, _, _ = train_test_split(segmented_sentences, segmented_sentences, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "sDHK8sk8bLGJ"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/content/drive/MyDrive/CEIA/nlp/models/segmented_sentences_train.json\", 'w') as file:\n",
    "    json.dump(segmented_sentences_train, file)\n",
    "\n",
    "with open(\"/content/drive/MyDrive/CEIA/nlp/models/segmented_sentences_val.json\", 'w') as file:\n",
    "    json.dump(segmented_sentences_val, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BO_sGPTqUPX0",
    "outputId": "32e41a35-69b6-4d86-961f-611fed5ac5f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9750, 1721)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(segmented_sentences_train), len(segmented_sentences_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jhAiIMQaUShE",
    "outputId": "ea904e51-372c-4081-ed00-f7ce29a07cf8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11471"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(segmented_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iD8moHGGpYF_"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zn-qStrtsQnr"
   },
   "source": [
    "Should we tokenize just in train set after splitting or in the whole dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oZBYIQ-bpPGH",
    "outputId": "cb0ca94d-dc50-4952-ea17-c5b9c5b066be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 730, 1, 4792, 40, 10, 6230, 102, 9233, 529, 3, 60, 770]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and fit the tokenizer with the segmented sentences\n",
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(segmented_sentences_train) # token 0: out of vocabulary words\n",
    "\n",
    "# Tokenize: Convert text to idx\n",
    "tokenized_sentences_train = tok.texts_to_sequences(segmented_sentences_train)\n",
    "tokenized_sentences_val = tok.texts_to_sequences(segmented_sentences_val)\n",
    "\n",
    "tokenized_sentences_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "IwdtrGqMa9Jl"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# saving\n",
    "with open('/content/drive/MyDrive/CEIA/nlp/models/20241123_model1/tok.pickle', 'wb') as handle:\n",
    "    pickle.dump(tok, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# # loading\n",
    "# with open('/content/drive/MyDrive/CEIA/nlp/models/20241122_model/tok.pickle', 'rb') as handle:\n",
    "#     tok = pickle.load(handle)\n",
    "\n",
    "# # Tokenize: Convert text to idx\n",
    "# tokenized_sentences_train = tok.texts_to_sequences(segmented_sentences_train)\n",
    "tokenized_sentences_val = tok.texts_to_sequences(segmented_sentences_val)\n",
    "\n",
    "# tokenized_sentences_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ny2WwkzyznNl"
   },
   "source": [
    "**Explore tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z5BUdO15p7Oe",
    "outputId": "08664a7b-12ad-4abb-a5d0-80b8ce9dfbb9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18512"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tok.word_counts)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WG_rjD4EqRS3",
    "outputId": "4d149323-4a02-4f5a-8bb2-cdf214d4b765"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('un', 2763),\n",
       " ('ranchito', 30),\n",
       " ('de', 9021),\n",
       " ('quincha', 3),\n",
       " ('solo', 583),\n",
       " ('me', 3079),\n",
       " ('ampara', 2),\n",
       " ('dos', 230),\n",
       " ('alientos', 1),\n",
       " ('amigos', 43)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of individual appearances per word\n",
    "list(tok.word_counts.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "00qCZFn5qd9v",
    "outputId": "f387ea7f-1186-457f-a17e-95709fe3874a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('la', 4567),\n",
       " ('ranchito', 29),\n",
       " ('alientos', 1),\n",
       " ('luna', 350),\n",
       " ('dos', 204),\n",
       " ('quincha', 3),\n",
       " ('un', 2151),\n",
       " ('solo', 519),\n",
       " ('de', 5531),\n",
       " ('amigos', 38)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of documents per word\n",
    "list(tok.word_docs.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "duvzvHZgqC03",
    "outputId": "f4627001-8f9e-42c0-cc36-0d04f9b43c25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('de', 1), ('que', 2), ('la', 3), ('y', 4), ('el', 5), ('en', 6), ('mi', 7), ('a', 8), ('no', 9), ('me', 10)]\n"
     ]
    }
   ],
   "source": [
    "# Idx of each word (ordered by popularity)\n",
    "print(list(tok.word_index.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QX_ZO03-tVLM"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jC2Htol3tCHz"
   },
   "source": [
    "**Create train set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "pjJYbgTcpnCe"
   },
   "outputs": [],
   "source": [
    "tok_sent = []\n",
    "\n",
    "max_len = max_context_size + 1 # we include the target\n",
    "for sent in tokenized_sentences_train:\n",
    "  # if sequence is longer than max_len, we create many subsequences\n",
    "  if len(sent) > max_len:\n",
    "    for i in range(len(sent) - max_len + 1):\n",
    "      tok_sent.append(sent[i : i + max_len])\n",
    "  else:\n",
    "    tok_sent.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LEHHHj4IwRZF"
   },
   "outputs": [],
   "source": [
    "# # example code to verify the functionality of the previous chunk\n",
    "# example_tok_sent = []\n",
    "\n",
    "# max_len = 8\n",
    "# for sent in [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]:\n",
    "#   if len(sent) > max_len:\n",
    "#     for i in range(len(sent) - max_len + 1):\n",
    "#       example_tok_sent.append(sent[i : i + max_len])\n",
    "#   else:\n",
    "#     example_tok_sent.append(sent)\n",
    "\n",
    "# print(example_tok_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KtIbGHS0pu6e",
    "outputId": "07a4c855-bd2b-4f99-8938-8c42eec8147a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16605"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tok_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0yLuFITlpv91",
    "outputId": "80beb565-3c72-4e0e-d69d-2fa8df24ef65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(154567, 36)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_sent_augm = []\n",
    "\n",
    "for sent in tok_sent:\n",
    "  # Generate every possible sub sequence and pad them to max_content_size + 1(target)\n",
    "  subseq = [\n",
    "      sent[:i + 2] # i + 2 to have at least 2 valid terms in the first sequence (word + target)\n",
    "      for i\n",
    "      in range(0, len(sent) - 1, 3)] # step = 3 to reduce training set~time\n",
    "  tok_sent_augm.append(pad_sequences(subseq, maxlen=max_context_size + 1, padding='pre'))\n",
    "\n",
    "# The previous step returned a list of list per sequence: concatenate them\n",
    "train_seqs = np.concatenate(tok_sent_augm, axis=0)\n",
    "\n",
    "train_seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "coUsZCy3xjYE",
    "outputId": "d53f0225-3847-4b56-de30-7a780c2a74af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  12, 730], dtype=int32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seqs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "8H4YKWqXp5bO"
   },
   "outputs": [],
   "source": [
    "# Get X and y arrays\n",
    "X = train_seqs[:, :-1]\n",
    "y = train_seqs[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oob5drQSzsJM"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVmBdg3f0rCN"
   },
   "source": [
    "**Create the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "aR7QCxHwqv2P",
    "outputId": "7492815b-edaf-460f-c562-e95f22f424e0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">296,208</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">12,544</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18513</span>)               │         <span style=\"color: #00af00; text-decoration-color: #00af00\">314,721</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m16\u001b[0m)              │         \u001b[38;5;34m296,208\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m64\u001b[0m)              │          \u001b[38;5;34m12,544\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │          \u001b[38;5;34m12,416\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                  │             \u001b[38;5;34m528\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18513\u001b[0m)               │         \u001b[38;5;34m314,721\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">636,417</span> (2.43 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m636,417\u001b[0m (2.43 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">636,417</span> (2.43 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m636,417\u001b[0m (2.43 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dropout, Dense\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding layer:\n",
    "# - Creates embeddings for vocabulary\n",
    "# - Adjustable dimensions for better representation\n",
    "model.add(Embedding(input_dim=vocab_size + 1, output_dim=16, input_shape=(max_context_size,)))\n",
    "\n",
    "# First and Second Bidirectional LSTM layer:\n",
    "# - Processes the output from the first layer\n",
    "# - Return sequences again to allow stacking further layers\n",
    "model.add(Bidirectional(LSTM(32, return_sequences=True, dropout=0.4, recurrent_dropout=0.2)))\n",
    "# model.add(Bidirectional(LSTM(32, return_sequences=True, dropout=0.3, recurrent_dropout=0.2)))\n",
    "\n",
    "# Third LSTM layer (unidirectional):\n",
    "# - Reduces dimensionality\n",
    "# - Does not return sequences\n",
    "model.add(LSTM(32, dropout=0.4))\n",
    "\n",
    "# Fully connected layer:\n",
    "# - Adds a dense layer for high-level feature learning\n",
    "model.add(Dense(16, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.5))  # Adds regularization\n",
    "\n",
    "# Output layer:\n",
    "# - Outputs probabilities for each class (softmax activation)\n",
    "# - Matches the vocabulary size for word prediction\n",
    "model.add(Dense(vocab_size + 1, activation='softmax', kernel_regularizer=l2(0.01)))\n",
    "\n",
    "# Compile the model:\n",
    "# - Sparse categorical crossentropy is used to handle integer-encoded targets\n",
    "model.compile(\n",
    "    loss=SparseCategoricalCrossentropy(),\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8EOtV2D0uHn"
   },
   "source": [
    "**Load previous model to continue training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "0uOlqQ4M2Edc"
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# # Load the saved model (replace the path with the correct file)\n",
    "# model = load_model('/content/drive/MyDrive/CEIA/nlp/models/20241123_model/model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Sb4c1Jc2bHv"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7IcnTA60yId"
   },
   "source": [
    "**Train model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "sc7ZEAoZOTyr"
   },
   "outputs": [],
   "source": [
    "np.savetxt('/content/drive/MyDrive/CEIA/nlp/models/X_train.txt', X, fmt='%d')\n",
    "np.savetxt('/content/drive/MyDrive/CEIA/nlp/models/y_train.txt', y, fmt='%d')\n",
    "\n",
    "# b = np.loadtxt('test1.txt', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "3mkV10APtHrQ"
   },
   "outputs": [],
   "source": [
    "class PplCallback(keras.callbacks.Callback):\n",
    "\n",
    "    '''\n",
    "    Este callback es una solución ad-hoc para calcular al final de cada epoch de\n",
    "    entrenamiento la métrica de Perplejidad sobre un conjunto de datos de validación.\n",
    "    La perplejidad es una métrica cuantitativa para evaluar la calidad de la generación de secuencias.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, val_data):\n",
    "      self.val_data = val_data\n",
    "\n",
    "      self.target = []\n",
    "      self.padded = []\n",
    "\n",
    "      count = 0\n",
    "      self.info = []\n",
    "\n",
    "      for seq in self.val_data:\n",
    "        len_seq = len(seq)\n",
    "        if len(seq) < 2:\n",
    "          continue\n",
    "\n",
    "        # We create all possible subsequences\n",
    "        subseq = [seq[:i] for i in range(1, len_seq)]\n",
    "        self.target.extend(seq[1:])\n",
    "        self.padded.append(pad_sequences(subseq, maxlen = max_context_size, padding='pre'))\n",
    "\n",
    "        self.info.append((count, count + (len_seq - 1)))\n",
    "        count += (len_seq - 1)\n",
    "\n",
    "      self.padded = np.vstack(self.padded)\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        scores = []\n",
    "        predictions = self.model.predict(self.padded, verbose=0)\n",
    "\n",
    "        predictions = np.clip(predictions, 1e-10, 1-1e-10) # Clip predictions to avoid log(0)\n",
    "\n",
    "        # Calculate perplexity score for each term\n",
    "        for start, end in self.info:\n",
    "            probs = [predictions[idx_seq, idx_vocab] for idx_seq, idx_vocab in zip(range(start, end), self.target[start : end])]\n",
    "            scores.append(np.exp(-np.mean(np.log(probs))))\n",
    "\n",
    "        # Calcualte mean perplexity\n",
    "        mean_ppl = np.mean(scores)\n",
    "        print(f'\\n mean perplexity: {mean_ppl} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6jC6SXWttgGW"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LambdaCallback, ModelCheckpoint\n",
    "\n",
    "# Define a callback to save the model every 5 epochs\n",
    "save_every_n_epochs = LambdaCallback(on_epoch_end=lambda epoch, logs:\n",
    "                                     model.save(f'/content/drive/MyDrive/CEIA/nlp/models/model_epoch_{epoch + 1}.keras', include_optimizer=True)\n",
    "                                     if (epoch + 1) % 1 == 0 else None)\n",
    "\n",
    "# Train the model with the custom callback\n",
    "hist = model.fit(\n",
    "    X,\n",
    "    y,\n",
    "    epochs=50,\n",
    "    callbacks=[\n",
    "        PplCallback(tokenized_sentences_val),  # Your existing callback\n",
    "        save_every_n_epochs                   # Save model every 5 epochs\n",
    "    ],\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLn0a2A_pvjH"
   },
   "source": [
    "    Epoch 1/50\n",
    "    2415/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 96ms/step - accuracy: 0.0360 - loss: 9.0349\n",
    "    mean perplexity: 1446.8643798828125\n",
    "\n",
    "    Epoch 2/50\n",
    "    2415/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 96ms/step - accuracy: 0.0396 - loss: 7.5174\n",
    "    mean perplexity: 1356.2362060546875\n",
    "\n",
    "    Epoch 3/50\n",
    "    2415/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 95ms/step - accuracy: 0.0397 - loss: 7.3744\n",
    "    mean perplexity: 1327.3463134765625\n",
    "\n",
    "    Epoch 4/50\n",
    "    2416/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 97ms/step - accuracy: 0.0395 - loss: 7.3188\n",
    "    mean perplexity: 1291.9498291015625\n",
    "\n",
    "    Epoch 5/50\n",
    "    2416/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 96ms/step - accuracy: 0.0399 - loss: 7.2244\n",
    "    mean perplexity: 1288.92822265625\n",
    "\n",
    "    Epoch 6/50\n",
    "    2415/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 96ms/step - accuracy: 0.0405 - loss: 7.1734\n",
    "    mean perplexity: 1282.300048828125\n",
    "\n",
    "    Epoch 7/50\n",
    "    2416/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 98ms/step - accuracy: 0.0403 - loss: 7.1300\n",
    "    mean perplexity: 1288.3594970703125\n",
    "\n",
    "    Epoch 8/50\n",
    "    2416/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 98ms/step - accuracy: 0.0409 - loss: 7.1032\n",
    "    mean perplexity: 1292.248046875\n",
    "\n",
    "    Epoch 9/50\n",
    "    2416/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 97ms/step - accuracy: 0.0408 - loss: 7.0814\n",
    "    mean perplexity: 1299.22216796875\n",
    "\n",
    "    Epoch 10/50\n",
    "    2416/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 97ms/step - accuracy: 0.0393 - loss: 7.0605\n",
    "    mean perplexity: 1319.4664306640625\n",
    "\n",
    "    Epoch 11/50\n",
    "    2416/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 98ms/step - accuracy: 0.0404 - loss: 7.0256\n",
    "    mean perplexity: 1327.1043701171875\n",
    "\n",
    "    Epoch 12/50\n",
    "    2416/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 98ms/step - accuracy: 0.0413 - loss: 7.0217\n",
    "    mean perplexity: 1345.6363525390625\n",
    "\n",
    "    Epoch 13/50\n",
    "    2416/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 101ms/step - accuracy: 0.0407 - loss: 6.9970\n",
    "    mean perplexity: 1371.47900390625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o2vbKfKwFuLk"
   },
   "outputs": [],
   "source": [
    "# Reload best model\n",
    "model = load_model('./../models/best_model.keras')\n",
    "\n",
    "# Recompile the model with the new optimizer\n",
    "# Should have saved optimizer state!\n",
    "model.compile(\n",
    "    loss=SparseCategoricalCrossentropy(),\n",
    "    optimizer=Adam(learning_rate=0.00003),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Retrain the model with the lower learning rate\n",
    "hist = model.fit(\n",
    "    X,\n",
    "    y,\n",
    "    epochs=50,\n",
    "    callbacks=[\n",
    "        PplCallback(tokenized_sentences_val),  # Your existing callback\n",
    "        save_every_n_epochs                   # Save model every 5 epochs\n",
    "    ],\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WsYB6jlQ241h"
   },
   "source": [
    "    Epoch 1/50\n",
    "    2415/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 100ms/step - accuracy: 0.0402 - loss: 7.1441\n",
    "    mean perplexity: 1300.7794189453125\n",
    "\n",
    "    Epoch 2/50\n",
    "    2416/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 99ms/step - accuracy: 0.0408 - loss: 7.1398\n",
    "    mean perplexity: 1291.698974609375\n",
    "\n",
    "    Epoch 3/50\n",
    "    2416/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 99ms/step - accuracy: 0.0408 - loss: 7.1261\n",
    "    mean perplexity: 1292.546630859375\n",
    "\n",
    "    Epoch 4/50\n",
    "    2416/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 99ms/step - accuracy: 0.0396 - loss: 7.1203\n",
    "    mean perplexity: 1294.7169189453125\n",
    "\n",
    "    Epoch 5/50\n",
    "    2416/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 100ms/step - accuracy: 0.0409 - loss: 7.0991\n",
    "    mean perplexity: 1290.9434814453125\n",
    "\n",
    "    Epoch 6/50\n",
    "    2415/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 99ms/step - accuracy: 0.0407 - loss: 7.0957\n",
    "    mean perplexity: 1289.7471923828125\n",
    "\n",
    "    Epoch 7/50\n",
    "    2415/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 100ms/step - accuracy: 0.0406 - loss: 7.0782\n",
    "    mean perplexity: 1296.2232666015625\n",
    "\n",
    "    Epoch 8/50\n",
    "    2415/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 99ms/step - accuracy: 0.0397 - loss: 7.0751\n",
    "    mean perplexity: 1299.64990234375\n",
    "\n",
    "    Epoch 9/50\n",
    "    2416/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 100ms/step - accuracy: 0.0410 - loss: 7.0653\n",
    "    mean perplexity: 1302.5528564453125\n",
    "\n",
    "    Epoch 10/50\n",
    "    2416/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 99ms/step - accuracy: 0.0398 - loss: 7.0735\n",
    "    mean perplexity: 1305.4849853515625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f8lh9aIu3BvI"
   },
   "outputs": [],
   "source": [
    "# Reload best model\n",
    "model = load_model('./../models/best_model.keras')\n",
    "\n",
    "# Recompile the model with the new optimizer\n",
    "# Should have saved optimizer state!\n",
    "model.compile(\n",
    "    loss=SparseCategoricalCrossentropy(),\n",
    "    optimizer=Adam(learning_rate=0.00001),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Retrain the model with the lower learning rate\n",
    "hist = model.fit(\n",
    "    X,\n",
    "    y,\n",
    "    epochs=50,\n",
    "    callbacks=[\n",
    "        PplCallback(tokenized_sentences_val),  # Your existing callback\n",
    "        save_every_n_epochs                   # Save model every 5 epochs\n",
    "    ],\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKJssSvSVRPE"
   },
   "source": [
    "    Epoch 1/50\n",
    "    2415/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 102ms/step - accuracy: 0.0397 - loss: 7.1501\n",
    "    mean perplexity: 1298.7662353515625\n",
    "\n",
    "    Epoch 2/50\n",
    "    2416/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 99ms/step - accuracy: 0.0409 - loss: 7.1265\n",
    "    mean perplexity: 1296.0526123046875\n",
    "\n",
    "    Epoch 3/50\n",
    "    2416/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 99ms/step - accuracy: 0.0409 - loss: 7.1437\n",
    "    mean perplexity: 1295.7974853515625\n",
    "\n",
    "    Epoch 4/50\n",
    "    2416/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 99ms/step - accuracy: 0.0401 - loss: 7.1375\n",
    "    mean perplexity: 1293.6080322265625\n",
    "\n",
    "    Epoch 5/50\n",
    "    2416/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 102ms/step - accuracy: 0.0407 - loss: 7.1347\n",
    "    mean perplexity: 1294.9107666015625\n",
    "\n",
    "    Epoch 6/50\n",
    "    2416/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 100ms/step - accuracy: 0.0408 - loss: 7.1304\n",
    "    mean perplexity: 1295.7454833984375\n",
    "\n",
    "    Epoch 7/50\n",
    "    2416/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 99ms/step - accuracy: 0.0409 - loss: 7.1180\n",
    "    mean perplexity: 1294.0455322265625\n",
    "\n",
    "    Epoch 8/50\n",
    "    2416/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 102ms/step - accuracy: 0.0414 - loss: 7.1190\n",
    "    mean perplexity: 1293.0714111328125\n",
    "\n",
    "    Epoch 9/50\n",
    "    2416/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 99ms/step - accuracy: 0.0407 - loss: 7.1065\n",
    "    mean perplexity: 1292.416748046875\n",
    "\n",
    "    Epoch 10/50\n",
    "    2416/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 100ms/step - accuracy: 0.0409 - loss: 7.1076\n",
    "    mean perplexity: 1293.7235107421875\n",
    "\n",
    "    Epoch 11/50\n",
    "    2416/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 100ms/step - accuracy: 0.0414 - loss: 7.1036\n",
    "    mean perplexity: 1292.7979736328125\n",
    "\n",
    "    Epoch 12/50\n",
    "    2416/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 99ms/step - accuracy: 0.0404 - loss: 7.0940\n",
    "    mean perplexity: 1292.5150146484375\n",
    "\n",
    "    Epoch 13/50\n",
    "    2416/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 99ms/step - accuracy: 0.0414 - loss: 7.0985\n",
    "    mean perplexity: 1292.34521484375\n",
    "\n",
    "    Epoch 14/50\n",
    "    2416/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 99ms/step - accuracy: 0.0399 - loss: 7.0878\n",
    "    mean perplexity: 1292.7373046875\n",
    "\n",
    "    Epoch 15/50\n",
    "    2416/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 99ms/step - accuracy: 0.0418 - loss: 7.0872\n",
    "    mean perplexity: 1291.9837646484375\n",
    "\n",
    "    Epoch 16/50\n",
    "    2416/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 100ms/step - accuracy: 0.0403 - loss: 7.0881\n",
    "    mean perplexity: 1292.9183349609375\n",
    "\n",
    "    Epoch 17/50\n",
    "    2415/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 100ms/step - accuracy: 0.0410 - loss: 7.0865\n",
    "    mean perplexity: 1293.0306396484375\n",
    "\n",
    "    Epoch 18/50\n",
    "    2416/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 100ms/step - accuracy: 0.0408 - loss: 7.0797\n",
    "    mean perplexity: 1294.1676025390625\n",
    "\n",
    "    Epoch 19/50\n",
    "    2415/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 99ms/step - accuracy: 0.0398 - loss: 7.0794\n",
    "    mean perplexity: 1294.05810546875\n",
    "\n",
    "    Epoch 20/50\n",
    "    2416/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 100ms/step - accuracy: 0.0411 - loss: 7.0828\n",
    "    mean perplexity: 1294.3927001953125\n",
    "\n",
    "    Epoch 21/50\n",
    "    2416/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 99ms/step - accuracy: 0.0408 - loss: 7.0656\n",
    "    mean perplexity: 1294.4691162109375\n",
    "\n",
    "    Epoch 22/50\n",
    "    2416/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 98ms/step - accuracy: 0.0405 - loss: 7.0654\n",
    "    mean perplexity: 1295.6854248046875\n",
    "\n",
    "    Epoch 23/50\n",
    "    2415/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 99ms/step - accuracy: 0.0410 - loss: 7.0790\n",
    "    mean perplexity: 1297.6031494140625\n",
    "\n",
    "    Epoch 24/50\n",
    "    2416/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 99ms/step - accuracy: 0.0408 - loss: 7.0695\n",
    "    mean perplexity: 1297.6300048828125\n",
    "\n",
    "    Epoch 25/50\n",
    "    2416/2416 ━━━━━━━━━━━━━━━━━━━━ 0s 99ms/step - accuracy: 0.0406 - loss: 7.0774\n",
    "    mean perplexity: 1297.540283203125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGxof03tZkef"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WaYCUXEOZlNL"
   },
   "source": [
    "**Test the best model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbe6FoPdaXBT"
   },
   "source": [
    "We select the model trained after epoch 6 in the first case which obtained the following metrics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBF5jhmRadp8"
   },
   "source": [
    "    train accuracy: 0.0405\n",
    "    train loss: 7.1734\n",
    "    val mean perplexity: 1282.300048828125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iqbo9TpLacTv"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "model = load_model('./../models/best_model.keras')\n",
    "\n",
    "with open('./../data/artifacts/tok.pickle', 'rb') as handle:\n",
    "    tok = pickle.load(handle)\n",
    "\n",
    "max_context_size = 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmlmG4NRc9dM"
   },
   "source": [
    "**Basic generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "id": "qk-IrhPqcwiM"
   },
   "outputs": [],
   "source": [
    "def generate_seq(model, tokenizer, seed_text, max_length, n_words):\n",
    "    \"\"\"\n",
    "        Exec model sequence prediction\n",
    "\n",
    "        Args:\n",
    "            model (keras): modelo entrenado\n",
    "            tokenizer (keras tokenizer): tonenizer utilizado en el preprocesamiento\n",
    "            seed_text (string): texto de entrada (input_seq)\n",
    "            max_length (int): máxima longitud de la sequencia de entrada\n",
    "            n_words (int): números de palabras a agregar a la sequencia de entrada\n",
    "        returns:\n",
    "            output_text (string): sentencia con las \"n_words\" agregadas\n",
    "    \"\"\"\n",
    "    output_text = seed_text\n",
    "\t# generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "\t\t# Encodeamos\n",
    "        encoded = tokenizer.texts_to_sequences([output_text])[0]\n",
    "\t\t# Si tienen distinto largo\n",
    "        encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "\n",
    "\t\t# Predicción softmax\n",
    "        y_hat = model.predict(encoded).argmax(axis=-1)\n",
    "\t\t# Vamos concatenando las predicciones\n",
    "        out_word = ''\n",
    "\n",
    "        # Debemos buscar en el vocabulario la palabra\n",
    "        # que corresopnde al indice (y_hat) predicho por le modelo\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == y_hat:\n",
    "                out_word = word\n",
    "                break\n",
    "\n",
    "\t\t# Agrego las palabras a la frase predicha\n",
    "        output_text += ' ' + out_word\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "9anRNcXdcxg8",
    "outputId": "4d51cbd7-9a42-4987-b9da-370e8a5f8e49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 646ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'chacarera de de de de de de de de de de'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text='chacarera'\n",
    "\n",
    "generate_seq(model, tok, input_text, max_length=max_context_size, n_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La inferencia deterministica elige siempre el termino mas representado en el set de entrenamiento. Vamos a probar usando beam search y muestreo aleatorio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aiv19POic_6k"
   },
   "source": [
    "**Beam search y muestreo aleatorio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "id": "d-zHIa1rdESr"
   },
   "outputs": [],
   "source": [
    "# funcionalidades para hacer encoding y decoding\n",
    "\n",
    "def encode(text,max_length=max_context_size):\n",
    "\n",
    "    encoded = tok.texts_to_sequences([text])[0]\n",
    "    encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "\n",
    "    return encoded\n",
    "\n",
    "def decode(seq):\n",
    "    return tok.sequences_to_texts([seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "id": "3VPxBiNMdFV8"
   },
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "def select_candidates(pred, num_beams, vocab_size, history_probs, history_tokens, temp=1):\n",
    "  # colectar todas las probabilidades para la siguiente búsqueda\n",
    "  pred_large = []\n",
    "  for idx,pp in enumerate(pred):\n",
    "    pred_large.extend(np.log(pp+1E-10) + history_probs[idx])\n",
    "\n",
    "  pred_large = np.array(pred_large)\n",
    "\n",
    "  # criterio de selección\n",
    "  # idx_select = np.argsort(pred_large)[::-1][:num_beams] # beam search determinista\n",
    "  idx_select = np.random.choice(\n",
    "      np.arange(pred_large.shape[0]),\n",
    "      num_beams,\n",
    "      p = softmax(pred_large / temp)\n",
    "      ) # beam search con muestreo\n",
    "\n",
    "  # traducir a índices de token en el vocabulario\n",
    "  new_history_tokens = np.concatenate((np.array(history_tokens)[idx_select//vocab_size],\n",
    "                        np.array([idx_select%vocab_size]).T),\n",
    "                      axis=1)\n",
    "\n",
    "  # devolver el producto de las probabilidades (log) y la secuencia de tokens seleccionados\n",
    "  return pred_large[idx_select.astype(int)], new_history_tokens.astype(int)\n",
    "\n",
    "\n",
    "def beam_search(model, num_beams, num_words, input, temp):\n",
    "    encoded = encode(input)\n",
    "    y_hat = np.squeeze(model.predict(encoded))\n",
    "\n",
    "    vocab_size = y_hat.shape[0]\n",
    "\n",
    "    history_probs = [0]*num_beams\n",
    "    history_tokens = [encoded[0]]*num_beams\n",
    "\n",
    "    # select num_beams candidates\n",
    "    history_probs, history_tokens = select_candidates(\n",
    "        pred = [y_hat],\n",
    "        num_beams = num_beams,\n",
    "        vocab_size = vocab_size,\n",
    "        history_probs = history_probs,\n",
    "        history_tokens = history_tokens,\n",
    "        temp = temp\n",
    "    )\n",
    "\n",
    "    # beam search loop\n",
    "    for i in range(num_words - 1):\n",
    "      preds = []\n",
    "      for hist in history_tokens:\n",
    "        # actualizar secuencia de tokens\n",
    "        input_update = np.array([hist[i+1:]]).copy()\n",
    "        # predicción\n",
    "        y_hat = np.squeeze(model.predict(input_update))\n",
    "        preds.append(y_hat)\n",
    "      history_probs, history_tokens = select_candidates(preds,\n",
    "                                                        num_beams,\n",
    "                                                        vocab_size,\n",
    "                                                        history_probs,\n",
    "                                                        history_tokens)\n",
    "    return history_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G4qfvd2hriUo"
   },
   "outputs": [],
   "source": [
    "input = \"cuando pienso en\"\n",
    "\n",
    "salidas = []\n",
    "for i in range(10):\n",
    "    salida = beam_search(\n",
    "        model,\n",
    "        num_beams=10,\n",
    "        num_words=6,\n",
    "        input=input,\n",
    "        temp=1\n",
    "    )\n",
    "\n",
    "    salidas.append(salida)\n",
    "    display.clear_output()\n",
    "\n",
    "for salida in salidas:\n",
    "    print(decode(salida[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1T8RI-2RxA9v"
   },
   "source": [
    "    ['cuando pienso en que tu soy de si vidalitass']\n",
    "    ['cuando pienso en amor que en la paloma iguana']\n",
    "    ['cuando pienso en y que solo a en coloradas']\n",
    "    ['cuando pienso en que que de en y de']\n",
    "    ['cuando pienso en amor es de el no soy']\n",
    "    ['cuando pienso en de que a y en me']\n",
    "    ['cuando pienso en el pero la con mi con']\n",
    "    ['cuando pienso en el la es y de traia']\n",
    "    ['cuando pienso en ni en el y para pero']\n",
    "    ['cuando pienso en que del un a luz con']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJGEGpQktlBO"
   },
   "source": [
    "Vemos que algunas pocas de las frases generadas tienden a tener cohesion, sin embargo vemos la repeticion constante de algunos terminos cortos (articulos y preposiciones principalmente). Vamos a ver a que se debe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZaVc8TZgt0Je",
    "outputId": "9826e7b1-7bf8-453f-c787-895fd718394d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  8,  7, 12,  9, 11, 10, 15, 14, 17, 13, 16,\n",
       "       19, 18, 22])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = encode(input)\n",
    "y_hat = np.squeeze(model.predict(encoded))\n",
    "\n",
    "idx = np.argsort(y_hat)[::-1][:20]\n",
    "idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WURF6e3IuAhW"
   },
   "source": [
    "Vemos que los terminos mas probables para nuestro input, y posiblemente para todos los que intentemos, son las palabras mas repetidas del tokenizador, es decir los indices menores (a excepcion del 0 que se reserva para vocablos no vistos en training).\n",
    "\n",
    "Puede ser de utilidad entonces incrementar la temperatura para que aparezcan terminos diferentes, pero sin excedernos en el valor para que las frases tiendan a conservar el sentido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qDrTULgbu3w-"
   },
   "outputs": [],
   "source": [
    "input = \"cuando pienso en\"\n",
    "temp = 1.5\n",
    "\n",
    "salidas = []\n",
    "for i in range(10):\n",
    "    salida = beam_search(\n",
    "        model,\n",
    "        num_beams=10,\n",
    "        num_words=6,\n",
    "        input=input,\n",
    "        temp=temp\n",
    "    )\n",
    "\n",
    "    salidas.append(salida)\n",
    "    display.clear_output()\n",
    "\n",
    "for salida in salidas:\n",
    "    print(decode(salida[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5m95pHDxOYn"
   },
   "source": [
    "    ['cuando pienso en las cancion en no la fe']\n",
    "    ['cuando pienso en y que en de no embriagara']\n",
    "    ['cuando pienso en sin corazon que su sus recordarlas']\n",
    "    ['cuando pienso en esta por y y sus siempe']\n",
    "    ['cuando pienso en tu hay de yo volver con']\n",
    "    ['cuando pienso en el que lo no de cocido']\n",
    "    ['cuando pienso en la con la como de una']\n",
    "    ['cuando pienso en pero y de en en si']\n",
    "    ['cuando pienso en ser en la pero de dormiremos']\n",
    "    ['cuando pienso en de de la que mi peliar']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BebMSVdswFUf"
   },
   "source": [
    "Se siguen percibiendo terminos cortos y repetidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1UWjGoaXvPx3"
   },
   "outputs": [],
   "source": [
    "input = \"cuando pienso en\"\n",
    "temp = 2\n",
    "\n",
    "salidas = []\n",
    "for i in range(10):\n",
    "    salida = beam_search(\n",
    "        model,\n",
    "        num_beams=10,\n",
    "        num_words=6,\n",
    "        input=input,\n",
    "        temp=temp\n",
    "    )\n",
    "\n",
    "    salidas.append(salida)\n",
    "    display.clear_output()\n",
    "\n",
    "for salida in salidas:\n",
    "    print(decode(salida[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8JDy6Hpxa1_"
   },
   "source": [
    "    ['cuando pienso en en el de el se nadie']\n",
    "    ['cuando pienso en estrella es de de desde cuando']\n",
    "    ['cuando pienso en pasar que es que en vivo']\n",
    "    ['cuando pienso en en amor no va de amenaza']\n",
    "    ['cuando pienso en mi para a de se eres']\n",
    "    ['cuando pienso en otra al con o la acorte']\n",
    "    ['cuando pienso en yo es es el casa las']\n",
    "    ['cuando pienso en decencia que el dolor que pico']\n",
    "    ['cuando pienso en sur con te de inutilmente chamarra']\n",
    "    ['cuando pienso en catedral por de a hombre hiciste']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9X7F7c-xcYv"
   },
   "source": [
    "Utilizando un patron de temperatura igual a 2 vemos una mejor relacion entre terminos y articulos. Incluso algunas de las oraciones muestran una cohesion decente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lcHszG7UxuD3"
   },
   "outputs": [],
   "source": [
    "input = \"cuando pienso en\"\n",
    "temp = 3\n",
    "\n",
    "salidas = []\n",
    "for i in range(10):\n",
    "    salida = beam_search(\n",
    "        model,\n",
    "        num_beams=10,\n",
    "        num_words=6,\n",
    "        input=input,\n",
    "        temp=temp\n",
    "    )\n",
    "\n",
    "    salidas.append(salida)\n",
    "    display.clear_output()\n",
    "\n",
    "for salida in salidas:\n",
    "    print(decode(salida[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PB2F8K0myBh_"
   },
   "source": [
    "    ['cuando pienso en vi de no cuando el y']\n",
    "    ['cuando pienso en vengo en y y por pago']\n",
    "    ['cuando pienso en vita y que tengo en es']\n",
    "    ['cuando pienso en los yo en hay la de']\n",
    "    ['cuando pienso en llame mi es y se otra']\n",
    "    ['cuando pienso en paseado mis yo mi que sus']\n",
    "    ['cuando pienso en secreto en se emocion sin martes']\n",
    "    ['cuando pienso en eran que que que que un']\n",
    "    ['cuando pienso en salio el y las su pruebas']\n",
    "    ['cuando pienso en vaca a que que el acullico']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SsJpWUz2yCoH"
   },
   "source": [
    "Con un valor de temperatura igual a 3 vemos que las oraciones pierden toda cohesion. Por lo que observamos, el mejor valor de temperatura es 2 para nuestro caso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gd9zjy0vnC8H"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-qUsyIJyT4Y"
   },
   "source": [
    "**Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5oRcon7FyVDr"
   },
   "source": [
    "Si bien el tamano de nuestro dataset se encuentra limitado por la capacidad de computo que podemos alcanzar, el entrenamiento a resultado un buen ejercicio para evaluar diferentes arquitecturas e hiperparametros para evitar el sobre-ajuste del modelo.\n",
    "\n",
    "La evaluacion de resultados arroja que el modelo tiende a priorizar preposiciones y articulos que se repiten constantemente en los documentos. Quizas para evitar esto se deberia buscar entrenar el modelo con un mejor equilibrio entre palabras cortas y largas en los documentos utilizados. O de algun modo priorizar terminos apareciendo mas esporadicamente durante el entrenamiento. Sin embargo, podemos observar un desempeno minimamente aceptable en algunas de las oraciones generados con un valor de temperatura igual a 2 (lo cual permite equilibrar un poco las probabilidades de los terminos y compensar la sobrerepresentacion de terminos cortos)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
